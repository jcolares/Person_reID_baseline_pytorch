{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/jcolares/Person_reID_baseline_pytorch/blob/master/test.-collab.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "eNFtb-9CBLHh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Person_reID_baseline_pytorch\n",
        "\n",
        "Baseline Code (with bottleneck) for Person-reID (pytorch).\n",
        "It is consistent with the new baseline result in \n",
        "[Beyond Part Models: Person Retrieval with Refined Part Pooling](https://arxiv.org/abs/1711.09349) and \n",
        "[Camera Style Adaptation for Person Re-identification](https://arxiv.org/abs/1711.10295).\n",
        "\n",
        "We arrived **Rank@1=88.24%, mAP=70.68%** only with softmax loss.\n",
        "\n",
        "Here we provide hyperparameters and architectures, that were used to generate the result. \n",
        "Some of them (i.e. learning rate) are far from optimal. Do not hesitate to change them and see the effect. \n",
        "\n",
        "P.S. With similar structure, we arrived **Rank@1=87.74% mAP=69.46%** with Matconvnet. (batchsize=8, dropout=0.75) \n",
        "You may refer to [Here](https://github.com/layumi/Person_reID_baseline_matconvnet).\n",
        "Different framework need to be tuned in a different way.\n",
        "\n",
        "**What's new:** Multiple-query Evaluation is added. The multiple-query result is about **Rank@1=91.95% mAP=78.06%**. \n",
        "```bash\n",
        "python prepare.py\n",
        "python train.py\n",
        "python test.py --multi\n",
        "python evaluate_gpu.py\n",
        "```\n",
        "\n",
        "**What's new:**  [PCB](https://arxiv.org/abs/1711.09349) is added. You may use '--PCB' to use this model. It can achieve around **Rank@1=92.73% mAP=78.16%**. I used a GPU (P40) with 16GB Memory. You may try apply smaller batchsize and choose the smaller learning rate (for stability) to run. \n",
        "```bash\n",
        "python train.py --PCB --batchsize 64 --name PCB-64\n",
        "python test.py --PCB --name PCB-64\n",
        "```\n",
        "\n",
        "**What's new:** You may try `evaluate_gpu.py` to conduct a faster evaluation with GPU.\n",
        "\n",
        "**What's new:** You may apply '--use_dense' to use `DenseNet-121`. It can easily arrive **Rank@1=89.91% mAP=73.58%**. Trained DenseNet-121 model can be found at [GoogleDrive](https://drive.google.com/open?id=1NgZWnYBCzESgKNzLeoWUMxggZ6SSEaZL).(Note that ResNet-50 is a more common choice as the baseline.)\n",
        "\n",
        "**What's new：** Trained ResNet-50 model is available at [GoogleDrive](https://drive.google.com/open?id=1__x0qNJ3T654wTghmuRjydn42NsAZW_M).\n",
        "\n",
        "**What's new:** Re-ranking is added to evaluation. The re-ranked result is **Rank@1=90.20% mAP=84.76%**.\n",
        "\n",
        "**What's new:** Random Erasing is added to train.\n",
        "\n",
        "**What's new:** I add some code to generate training curves. The figure will be saved into the model folder when training.\n",
        "\n",
        "![](https://github.com/layumi/Person_reID_baseline_pytorch/blob/master/train.jpg)\n",
        "\n",
        "## Model Structure\n",
        "You may learn more from `model.py`. \n",
        "We add one linear layer(bottleneck), one batchnorm layer and relu.\n",
        "\n",
        "## Prerequisites\n",
        "\n",
        "- Python 3.6\n",
        "- GPU Memory >= 6G\n",
        "- Numpy\n",
        "- Pytorch 0.3+\n",
        "\n",
        "**(Some reports found that updating numpy can arrive the right accuracy. If you only get 50~80 Top1 Accuracy, just try it.)**\n",
        "We have successfully run the code based on numpy 1.12.1 and 1.13.1 .\n",
        "\n",
        "## Getting started\n",
        "### Installation\n",
        "- Install Pytorch from http://pytorch.org/\n",
        "- Install Torchvision from the source\n",
        "```\n",
        "git clone https://github.com/pytorch/vision\n",
        "cd vision\n",
        "python setup.py install\n",
        "```\n",
        "Because pytorch and torchvision are ongoing projects.\n",
        "\n",
        "Here we noted that our code is tested based on Pytorch 0.3.0/0.4.0 and Torchvision 0.2.0.\n",
        "\n",
        "## Dataset & Preparation\n",
        "Download [Market1501 Dataset](http://www.liangzheng.org/Project/project_reid.html)\n",
        "\n",
        "Preparation: Put the images with the same id in one folder. You may use \n",
        "```bash\n",
        "python prepare.py\n",
        "```\n",
        "Remember to change the dataset path to your own path.\n",
        "\n",
        "Futhermore, you also can test our code on [DukeMTMC-reID Dataset](https://github.com/layumi/DukeMTMC-reID_evaluation).\n",
        "Our baseline code is not such high on DukeMTMC-reID **Rank@1=64.23%, mAP=43.92%**. Hyperparameters are need to be tuned.\n",
        "\n",
        "To save trained model, we make a dir.\n",
        "```bash\n",
        "mkdir model \n",
        "```\n",
        "\n",
        "## Train\n",
        "Train a model by\n",
        "```bash\n",
        "python train.py --gpu_ids 0 --name ft_ResNet50 --train_all --batchsize 32  --data_dir your_data_path\n",
        "```\n",
        "`--gpu_ids` which gpu to run.\n",
        "\n",
        "`--name` the name of model.\n",
        "\n",
        "`--data_dir` the path of the training data.\n",
        "\n",
        "`--train_all` using all images to train. \n",
        "\n",
        "`--batchsize` batch size.\n",
        "\n",
        "`--erasing_p` random erasing probability.\n",
        "\n",
        "Train a model with random erasing by\n",
        "```bash\n",
        "python train.py --gpu_ids 0 --name ft_ResNet50 --train_all --batchsize 32  --data_dir your_data_path --erasing_p 0.5\n",
        "```\n",
        "\n",
        "## Test\n",
        "Use trained model to extract feature by\n",
        "```bash\n",
        "python test.py --gpu_ids 0 --name ft_ResNet50 --test_dir your_data_path  --which_epoch 59\n",
        "```\n",
        "`--gpu_ids` which gpu to run.\n",
        "\n",
        "`--name` the dir name of trained model.\n",
        "\n",
        "`--which_epoch` select the i-th model.\n",
        "\n",
        "`--data_dir` the path of the testing data.\n",
        "\n",
        "\n",
        "## Evaluation\n",
        "```bash\n",
        "python evaluate.py\n",
        "```\n",
        "It will output Rank@1, Rank@5, Rank@10 and mAP results.\n",
        "You may also try `evaluate_gpu.py` to conduct a faster evaluation with GPU.\n",
        "\n",
        "For mAP calculation, you also can refer to the [C++ code for Oxford Building](http://www.robots.ox.ac.uk/~vgg/data/oxbuildings/compute_ap.cpp). We use the triangle mAP calculation (consistent with the Market1501 original code).\n",
        "\n",
        "### re-ranking\n",
        "```bash\n",
        "python evaluate_rerank.py\n",
        "```\n",
        "**It may take more than 10G Memory to run.** So run it on a powerful machine if possible. \n",
        "\n",
        "It will output Rank@1, Rank@5, Rank@10 and mAP results.\n",
        "\n",
        "## Ablation Study\n",
        "The model is based on Resnet50. Input images are resized to 256x128.\n",
        "Here we just show some results.\n",
        "\n",
        "**Note that the result may contain around 1% bias.(For example, 50th-epoch model can be better.)**\n",
        "\n",
        "| BatchSize | Dropout | Rank@1 | mAP | Note|\n",
        "| --------- | -------- | ----- | ---- | ---- |\n",
        "| 16 | 0.5  | 86.67 | 68.19 | |\n",
        "| 32 | 0.5  | 87.98 | 69.38 | |\n",
        "| 32 | 0.5  | **88.24** | **70.68** | test with 288x144|\n",
        "| 32 | 0.5  | **89.13** | **73.50** | train with random erasing and test with 288x144|\n",
        "| 32 | 0.5  | 87.14 | 68.90 | 0.1 color jitter|\n",
        "| 64 | 0.5  | 86.82 | 67.48 | |\n",
        "| 64 | 0.5  | 85.78 | 65.97 | 0.1 color jitter|\n",
        "| 64 | 0.5  | 85.42 | 65.29 | 0.4 color jitter|\n",
        "| 64 | 0.75 | 84.86 | 66.06 | |\n",
        "| 96 | 0.5  | 86.05 | 67.03 | |\n",
        "| 96 | 0.75 | 85.66 | 66.44 | |\n",
        "\n",
        "### Bottleneck\n",
        "Test with 144x288, dropout rate is 0.5\n",
        "\n",
        "| BatchSize | Bottleneck | Rank@1 | mAP | Note|\n",
        "| --------- | ---------- | ------ | --- | ---- |\n",
        "| 32 | 256  | 87.26 | 69.92 | |\n",
        "| 32 | 512  | **88.24** | **70.68** | |\n",
        "| 32 | 1024 | 84.29 | 64.00 | |\n",
        "\n",
        "\n",
        "## Citation\n",
        "As far as I know, the following papers may be the first two to use the bottleneck baseline. You may cite them in your paper.\n",
        "```\n",
        "@article{DBLP:journals/corr/SunZDW17,\n",
        "  author    = {Yifan Sun and\n",
        "               Liang Zheng and\n",
        "               Weijian Deng and\n",
        "               Shengjin Wang},\n",
        "  title     = {SVDNet for Pedestrian Retrieval},\n",
        "  booktitle   = {ICCV},\n",
        "  year      = {2017},\n",
        "}\n",
        "\n",
        "@article{hermans2017defense,\n",
        "  title={In Defense of the Triplet Loss for Person Re-Identification},\n",
        "  author={Hermans, Alexander and Beyer, Lucas and Leibe, Bastian},\n",
        "  journal={arXiv preprint arXiv:1703.07737},\n",
        "  year={2017}\n",
        "}\n",
        "```\n",
        "\n",
        "## Related Repos\n",
        "1. [Pedestrian Alignment Network](https://github.com/layumi/Pedestrian_Alignment)\n",
        "2. [2stream Person re-ID](https://github.com/layumi/2016_person_re-ID)\n",
        "3. [Pedestrian GAN](https://github.com/layumi/Person-reID_GAN)\n",
        "4. [Language Person Search](https://github.com/layumi/Image-Text-Embedding)"
      ]
    },
    {
      "metadata": {
        "id": "L3Xt1b0DB01B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# http://pytorch.org/\n",
        "from os import path\n",
        "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "\n",
        "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
        "\n",
        "!pip install -q http://download.pytorch.org/whl/{accelerator}/torch-0.3.0.post4-{platform}-linux_x86_64.whl torchvision\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "85-s0S_OCv71",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 863
        },
        "outputId": "c9c201ba-354c-4ff8-d535-494221099161"
      },
      "cell_type": "code",
      "source": [
        "# https://www.scipy.org/\n",
        "!apt-get -qq install python-scipy"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Selecting previously unselected package libjbig0:amd64.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 18396 files and directories currently installed.)\r\n",
            "Preparing to unpack .../00-libjbig0_2.1-3.1_amd64.deb ...\r\n",
            "Unpacking libjbig0:amd64 (2.1-3.1) ...\n",
            "Selecting previously unselected package liblcms2-2:amd64.\n",
            "Preparing to unpack .../01-liblcms2-2_2.7-1ubuntu1_amd64.deb ...\n",
            "Unpacking liblcms2-2:amd64 (2.7-1ubuntu1) ...\n",
            "Selecting previously unselected package libtiff5:amd64.\n",
            "Preparing to unpack .../02-libtiff5_4.0.8-5ubuntu0.1_amd64.deb ...\n",
            "Unpacking libtiff5:amd64 (4.0.8-5ubuntu0.1) ...\n",
            "Selecting previously unselected package libwebp6:amd64.\n",
            "Preparing to unpack .../03-libwebp6_0.6.0-3_amd64.deb ...\n",
            "Unpacking libwebp6:amd64 (0.6.0-3) ...\n",
            "Selecting previously unselected package libwebpmux3:amd64.\n",
            "Preparing to unpack .../04-libwebpmux3_0.6.0-3_amd64.deb ...\n",
            "Unpacking libwebpmux3:amd64 (0.6.0-3) ...\n",
            "Selecting previously unselected package python-decorator.\n",
            "Preparing to unpack .../05-python-decorator_4.1.1-1_all.deb ...\n",
            "Unpacking python-decorator (4.1.1-1) ...\n",
            "Selecting previously unselected package python-olefile.\n",
            "Preparing to unpack .../06-python-olefile_0.44-1_all.deb ...\n",
            "Unpacking python-olefile (0.44-1) ...\n",
            "Selecting previously unselected package python-pil:amd64.\n",
            "Preparing to unpack .../07-python-pil_4.1.1-3build2_amd64.deb ...\n",
            "Unpacking python-pil:amd64 (4.1.1-3build2) ...\n",
            "Selecting previously unselected package python-imaging.\n",
            "Preparing to unpack .../08-python-imaging_4.1.1-3build2_all.deb ...\n",
            "Unpacking python-imaging (4.1.1-3build2) ...\n",
            "Selecting previously unselected package python-numpy.\n",
            "Preparing to unpack .../09-python-numpy_1%3a1.12.1-3.1ubuntu4_amd64.deb ...\n",
            "Unpacking python-numpy (1:1.12.1-3.1ubuntu4) ...\n",
            "Selecting previously unselected package python-scipy.\n",
            "Preparing to unpack .../10-python-scipy_0.18.1-2ubuntu5_amd64.deb ...\n",
            "Unpacking python-scipy (0.18.1-2ubuntu5) ...\n",
            "Setting up liblcms2-2:amd64 (2.7-1ubuntu1) ...\n",
            "Setting up libjbig0:amd64 (2.1-3.1) ...\n",
            "Setting up libtiff5:amd64 (4.0.8-5ubuntu0.1) ...\n",
            "Setting up python-numpy (1:1.12.1-3.1ubuntu4) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n",
            "Setting up python-olefile (0.44-1) ...\n",
            "Setting up python-decorator (4.1.1-1) ...\n",
            "Setting up libwebp6:amd64 (0.6.0-3) ...\n",
            "Setting up python-scipy (0.18.1-2ubuntu5) ...\n",
            "Setting up libwebpmux3:amd64 (0.6.0-3) ...\n",
            "Setting up python-pil:amd64 (4.1.1-3build2) ...\n",
            "Setting up python-imaging (4.1.1-3build2) ...\n",
            "Processing triggers for libc-bin (2.26-0ubuntu2.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "AlInTdhJBLHi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "outputId": "af808897-009b-4f05-c160-316c424f4dbb"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function, division\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import os\n",
        "import scipy.io\n",
        "from model import ft_net, ft_net_dense, PCB, PCB_test"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b4e656af7cd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mft_net\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mft_net_dense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPCB_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "n49f2lKLBLHm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "######################################################################\n",
        "# Options\n",
        "# --------\n",
        "parser = argparse.ArgumentParser(description='Training')\n",
        "parser.add_argument('--gpu_ids',default='0', type=str,help='gpu_ids: e.g. 0  0,1,2  0,2')\n",
        "parser.add_argument('--which_epoch',default='last', type=str, help='0,1,2,3...or last')\n",
        "parser.add_argument('--test_dir',default='/home/zzd/Market/pytorch',type=str, help='./test_data')\n",
        "parser.add_argument('--name', default='ft_ResNet50', type=str, help='save model path')\n",
        "parser.add_argument('--batchsize', default=100, type=int, help='batchsize')\n",
        "parser.add_argument('--use_dense', action='store_true', help='use densenet121' )\n",
        "parser.add_argument('--PCB', action='store_true', help='use PCB' )\n",
        "parser.add_argument('--multi', action='store_true', help='use multiple query' )"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}